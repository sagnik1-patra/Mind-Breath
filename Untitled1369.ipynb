{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a715919d-cfa4-4d3a-ac38-2d2b3fbefadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.5426 - loss: 1.1018 - val_accuracy: 0.9023 - val_loss: 0.4550\n",
      "Epoch 2/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8897 - loss: 0.4379 - val_accuracy: 0.9049 - val_loss: 0.2742\n",
      "Epoch 3/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9294 - loss: 0.2564 - val_accuracy: 0.9100 - val_loss: 0.2186\n",
      "Epoch 4/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9345 - loss: 0.1952 - val_accuracy: 0.9100 - val_loss: 0.2024\n",
      "Epoch 5/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9374 - loss: 0.1733 - val_accuracy: 0.9177 - val_loss: 0.1879\n",
      "Epoch 6/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9391 - loss: 0.1580 - val_accuracy: 0.9229 - val_loss: 0.1769\n",
      "Epoch 7/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9377 - loss: 0.1452 - val_accuracy: 0.9254 - val_loss: 0.1712\n",
      "Epoch 8/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9454 - loss: 0.1358 - val_accuracy: 0.9254 - val_loss: 0.1695\n",
      "Epoch 9/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9445 - loss: 0.1312 - val_accuracy: 0.9280 - val_loss: 0.1667\n",
      "Epoch 10/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9511 - loss: 0.1210 - val_accuracy: 0.9280 - val_loss: 0.1612\n",
      "Epoch 11/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9430 - loss: 0.1264 - val_accuracy: 0.9280 - val_loss: 0.1606\n",
      "Epoch 12/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9451 - loss: 0.1250 - val_accuracy: 0.9254 - val_loss: 0.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] H5 -> C:\\Users\\sagni\\Downloads\\Mind Breath\\mindbreath_model.h5\n",
      "[SAVE] JSON -> C:\\Users\\sagni\\Downloads\\Mind Breath\\mindbreath_model.json\n",
      "[SAVE] YAML -> C:\\Users\\sagni\\Downloads\\Mind Breath\\mindbreath_model.yaml\n",
      "[SAVE] PKL -> C:\\Users\\sagni\\Downloads\\Mind Breath\\mindbreath_preprocess.pkl\n",
      "\n",
      "=== Training complete ===\n",
      "Target column: stress_level\n",
      "Detected -> numeric: ['Academic and extracurricular activities conflicting for you?', 'Age', 'Are you facing any difficulties with your professors or instructors?', 'Are you in competition with your peers, and does it affect you?', 'Do you attend classes regularly?', 'Do you face any sleep problems or difficulties falling asleep?', 'Do you feel overwhelmed with your academic workload?', 'Do you find that your relationship often causes you stress?', 'Do you get irritated easily?', 'Do you have trouble concentrating on your academic tasks?', 'Do you lack confidence in your academic performance?', 'Do you lack confidence in your choice of academic subjects?', 'Do you often feel lonely or isolated?', 'Gender', 'Have you been dealing with anxiety or tension recently?', 'Have you been dealing with anxiety or tension recently?.1', 'Have you been experiencing any illness or health issues?', 'Have you been feeling sadness or low mood?', 'Have you been getting headaches more often than usual?', 'Have you gained/lost weight?', 'Have you noticed a rapid heartbeat or palpitations?', 'Have you recently experienced stress in your life?', 'Is your hostel or home environment causing you difficulties?', 'Is your working environment unpleasant or stressful?', 'academic_performance', 'anxiety_level', 'basic_needs', 'blood_pressure', 'breathing_problem', 'bullying', 'depression', 'extracurricular_activities', 'future_career_concerns', 'headache', 'living_conditions', 'mental_health_history', 'noise_level', 'peer_pressure', 'safety', 'self_esteem', 'sleep_quality', 'social_support', 'study_load', 'teacher_student_relationship'] | categorical: ['Which type of stress do you primarily experience?'] | text: [] | datetime: ['Do you struggle to find time for relaxation and leisure activities?']\n",
      "Validation accuracy: 0.9254\n",
      "Artifacts saved in: C:\\Users\\sagni\\Downloads\\Mind Breath\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "CSV1 = r\"C:\\Users\\sagni\\Downloads\\Mind Breath\\archive\\Stress_Dataset.csv\"\n",
    "CSV2 = r\"C:\\Users\\sagni\\Downloads\\Mind Breath\\archive\\StressLevelDataset.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\sagni\\Downloads\\Mind Breath\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper transformers (picklable; no lambdas)\n",
    "# -----------------------------\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Select a single column as a 2D DataFrame.\"\"\"\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.column]]\n",
    "\n",
    "class To1DString(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert a 2D array/DataFrame (n,1) to 1D array[str] for text vectorizers.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            arr = X.iloc[:, 0].astype(str).values\n",
    "        else:\n",
    "            arr = np.asarray(X).astype(str).ravel()\n",
    "        return arr\n",
    "\n",
    "class DateTimeExpand(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Expand datetime columns into year/month/day/dow/hour features.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.out_cols = []\n",
    "    def fit(self, X, y=None):\n",
    "        self.out_cols = []\n",
    "        for c in self.columns:\n",
    "            self.out_cols += [f\"{c}_year\", f\"{c}_month\", f\"{c}_day\", f\"{c}_dow\", f\"{c}_hour\"]\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        outs = []\n",
    "        for c in self.columns:\n",
    "            s = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "            outs.append(pd.DataFrame({\n",
    "                f\"{c}_year\":  s.dt.year.fillna(0).astype(int),\n",
    "                f\"{c}_month\": s.dt.month.fillna(0).astype(int),\n",
    "                f\"{c}_day\":   s.dt.day.fillna(0).astype(int),\n",
    "                f\"{c}_dow\":   s.dt.dayofweek.fillna(0).astype(int),\n",
    "                f\"{c}_hour\":  s.dt.hour.fillna(0).astype(int),\n",
    "            }))\n",
    "        return pd.concat(outs, axis=1) if outs else np.empty((len(X), 0))\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def get_ohe_version_safe():\n",
    "    \"\"\"Return OneHotEncoder with the right sparse arg for current scikit-learn.\"\"\"\n",
    "    try:\n",
    "        # sklearn >= 1.4\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        # sklearn < 1.4\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def to_dense_if_reasonable(X, max_feats=50000):\n",
    "    \"\"\"Densify sparse matrix only if feature count is modest to avoid RAM blow-ups.\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        return X.toarray() if X.shape[1] <= max_feats else X\n",
    "    return X\n",
    "\n",
    "def plot_curve(hist_df, y_series, y_val_series, title, ylabel, out_path):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(hist_df.index, y_series, label=\"Train\")\n",
    "    if y_val_series is not None:\n",
    "        plt.plot(hist_df.index, y_val_series, label=\"Validation\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def load_and_align(csv_paths):\n",
    "    \"\"\"Load multiple CSVs, align columns by union, and concatenate.\"\"\"\n",
    "    dfs = []\n",
    "    all_cols = set()\n",
    "    for p in csv_paths:\n",
    "        d = pd.read_csv(p)\n",
    "        dfs.append(d)\n",
    "        all_cols |= set(d.columns)\n",
    "    # Ensure consistent column order (sorted for stability)\n",
    "    all_cols = list(sorted(all_cols))\n",
    "    aligned = []\n",
    "    for d in dfs:\n",
    "        missing = [c for c in all_cols if c not in d.columns]\n",
    "        if missing:\n",
    "            for m in missing:\n",
    "                d[m] = np.nan\n",
    "        aligned.append(d[all_cols])\n",
    "    return pd.concat(aligned, axis=0, ignore_index=True)\n",
    "\n",
    "def detect_target_column(df):\n",
    "    \"\"\"Heuristically detect a stress label column.\"\"\"\n",
    "    candidates = [\n",
    "        \"stress\", \"stress_level\", \"stresslevel\", \"label\", \"class\", \"target\", \"y\",\n",
    "        \"stress category\", \"stress_category\", \"stress level\"\n",
    "    ]\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for key in candidates:\n",
    "        if key in lower_map:\n",
    "            return lower_map[key]\n",
    "    return df.columns[-1]  # fallback\n",
    "\n",
    "# -----------------------------\n",
    "# Load & combine data\n",
    "# -----------------------------\n",
    "df = load_and_align([CSV1, CSV2])\n",
    "\n",
    "# Try to coerce obvious datetime-like columns (by name)\n",
    "for c in df.columns:\n",
    "    lc = c.lower()\n",
    "    if any(k in lc for k in [\"date\", \"time\", \"timestamp\", \"datetime\"]):\n",
    "        try:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "target_col = detect_target_column(df)\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y_raw = df[target_col]\n",
    "\n",
    "# -----------------------------\n",
    "# Column typing (auto-detect)\n",
    "# -----------------------------\n",
    "numeric_cols   = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "obj_cols       = X_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "datetime_cols  = X_df.select_dtypes(include=[\"datetime64[ns]\"]).columns.tolist()\n",
    "\n",
    "# Attempt to parse object columns that look like timestamps\n",
    "for c in list(set(X_df.columns) - set(numeric_cols) - set(datetime_cols)):\n",
    "    try:\n",
    "        parsed = pd.to_datetime(X_df[c], errors=\"raise\")\n",
    "        ok_ratio = parsed.notna().mean()\n",
    "        if ok_ratio > 0.7:\n",
    "            X_df[c] = parsed\n",
    "            datetime_cols.append(c)\n",
    "            if c in obj_cols:\n",
    "                obj_cols.remove(c)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Heuristic: long strings => text, short strings => categorical\n",
    "text_cols, cat_cols = [], []\n",
    "for c in obj_cols:\n",
    "    s = X_df[c].astype(str)\n",
    "    (text_cols if s.str.len().mean() > 40 else cat_cols).append(c)\n",
    "\n",
    "# -----------------------------\n",
    "# Build preprocessing (picklable; version-safe OHE; no lambdas)\n",
    "# -----------------------------\n",
    "transformers = []\n",
    "\n",
    "# Numeric\n",
    "if numeric_cols:\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    transformers.append((\"num\", num_pipe, numeric_cols))\n",
    "\n",
    "# Categorical (with version-safe OHE)\n",
    "if cat_cols:\n",
    "    ohe = get_ohe_version_safe()\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\",  ohe)\n",
    "    ])\n",
    "    transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "# Text (TF-IDF per text column)\n",
    "for c in text_cols:\n",
    "    txt_pipe = Pipeline([\n",
    "        (\"select\",  ColumnSelector(c)),\n",
    "        (\"impute\",  SimpleImputer(strategy=\"constant\", fill_value=\"\")),\n",
    "        (\"to1d\",    To1DString()),\n",
    "        (\"tfidf\",   TfidfVectorizer(max_features=8000))\n",
    "    ])\n",
    "    transformers.append((f\"text_{c}\", txt_pipe, [c]))\n",
    "\n",
    "# Datetime expansion\n",
    "if datetime_cols:\n",
    "    dt_pipe = Pipeline([\n",
    "        (\"expand\",  DateTimeExpand(datetime_cols)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler())\n",
    "    ])\n",
    "    transformers.append((\"dt\", dt_pipe, datetime_cols))\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=transformers, sparse_threshold=0.3) if transformers else \"passthrough\"\n",
    "\n",
    "# -----------------------------\n",
    "# Encode target + split\n",
    "# -----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw.astype(str))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X_df, y, test_size=0.2, random_state=42, stratify=y if n_classes > 1 else None\n",
    ")\n",
    "\n",
    "# Fit & transform\n",
    "if preprocess == \"passthrough\":\n",
    "    X_train = X_train_df.values\n",
    "    X_test  = X_test_df.values\n",
    "else:\n",
    "    X_train = preprocess.fit_transform(X_train_df, y_train)\n",
    "    X_test  = preprocess.transform(X_test_df)\n",
    "\n",
    "# Densify only if reasonable\n",
    "X_train = to_dense_if_reasonable(X_train)\n",
    "X_test  = to_dense_if_reasonable(X_test)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# -----------------------------\n",
    "# Build & train Keras model\n",
    "# -----------------------------\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(input_dim, n_classes):\n",
    "    m = Sequential()\n",
    "    m.add(Dense(256, activation=\"relu\", input_shape=(input_dim,)))\n",
    "    m.add(Dropout(0.35))\n",
    "    m.add(Dense(128, activation=\"relu\"))\n",
    "    m.add(Dropout(0.25))\n",
    "    if n_classes <= 2:\n",
    "        m.add(Dense(1, activation=\"sigmoid\"))\n",
    "        m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        m.add(Dense(n_classes, activation=\"softmax\"))\n",
    "        m.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "model = build_model(input_dim=input_dim, n_classes=n_classes)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=12,          # adjust if needed\n",
    "    batch_size=256,     # tune to your RAM/VRAM\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Save training curves\n",
    "# -----------------------------\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.index = np.arange(1, len(hist_df) + 1)\n",
    "hist_csv = os.path.join(OUT_DIR, \"training_history.csv\")\n",
    "hist_df.to_csv(hist_csv, index_label=\"epoch\")\n",
    "\n",
    "acc_key = \"accuracy\" if \"accuracy\" in hist_df.columns else (\"acc\" if \"acc\" in hist_df.columns else None)\n",
    "val_acc_key = \"val_accuracy\" if \"val_accuracy\" in hist_df.columns else (\"val_acc\" if \"val_acc\" in hist_df.columns else None)\n",
    "\n",
    "if acc_key:\n",
    "    plot_curve(hist_df,\n",
    "               hist_df[acc_key],\n",
    "               hist_df[val_acc_key] if val_acc_key in hist_df.columns else None,\n",
    "               \"Model Accuracy\", \"Accuracy\",\n",
    "               os.path.join(OUT_DIR, \"accuracy.png\"))\n",
    "\n",
    "plot_curve(hist_df,\n",
    "           hist_df[\"loss\"],\n",
    "           hist_df[\"val_loss\"] if \"val_loss\" in hist_df.columns else None,\n",
    "           \"Model Loss\", \"Loss\",\n",
    "           os.path.join(OUT_DIR, \"loss.png\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Save artifacts\n",
    "# -----------------------------\n",
    "# 1) Keras H5 (weights+graph)\n",
    "h5_path = os.path.join(OUT_DIR, \"mindbreath_model.h5\")\n",
    "model.save(h5_path)\n",
    "print(f\"[SAVE] H5 -> {h5_path}\")\n",
    "\n",
    "# 2) Model config JSON\n",
    "json_path = os.path.join(OUT_DIR, \"mindbreath_model.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model.get_config(), f, indent=2)\n",
    "print(f\"[SAVE] JSON -> {json_path}\")\n",
    "\n",
    "# 3) Model config YAML\n",
    "yaml_path = os.path.join(OUT_DIR, \"mindbreath_model.yaml\")\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(model.get_config(), f, sort_keys=False)\n",
    "print(f\"[SAVE] YAML -> {yaml_path}\")\n",
    "\n",
    "# 4) Preprocess bundle PKL (picklable)\n",
    "pkl_path = os.path.join(OUT_DIR, \"mindbreath_preprocess.pkl\")\n",
    "joblib.dump({\n",
    "    \"preprocess\": preprocess,\n",
    "    \"label_encoder\": label_encoder,\n",
    "    \"target_col\": target_col,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"text_cols\": text_cols,\n",
    "    \"datetime_cols\": datetime_cols\n",
    "}, pkl_path, compress=3, protocol=4)\n",
    "print(f\"[SAVE] PKL -> {pkl_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final log\n",
    "# -----------------------------\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "if n_classes <= 2:\n",
    "    y_pred = (y_pred_prob.ravel() >= 0.5).astype(int)\n",
    "else:\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\n=== Training complete ===\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Detected -> numeric: {numeric_cols} | categorical: {cat_cols} | text: {text_cols} | datetime: {datetime_cols}\")\n",
    "print(f\"Validation accuracy: {acc:.4f}\")\n",
    "print(f\"Artifacts saved in: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd3121-32d8-4120-8238-ad0e0aa3027c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
