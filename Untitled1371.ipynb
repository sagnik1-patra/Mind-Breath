{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f88595-7923-48e1-9279-8769179c6028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence to predict (or leave blank to use a CSV):  \n",
      "Enter path to input CSV (or press Enter to cancel):  C:\\Users\\sagni\\Downloads\\Mind Breath\\archive\\Stress_Dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Predictions -> C:\\Users\\sagni\\Downloads\\Mind Breath\\predictions_mindbreath.csv\n"
     ]
    }
   ],
   "source": [
    "# === predict_mindbreath.py ===\n",
    "# MindBreath: Prediction script (CLI + Jupyter safe), CSV or single-text.\n",
    "\n",
    "import os, sys, argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "OUT_DIR    = r\"C:\\Users\\sagni\\Downloads\\Mind Breath\"\n",
    "PKL_PATH   = os.path.join(OUT_DIR, \"mindbreath_preprocess.pkl\")\n",
    "H5_PATH    = os.path.join(OUT_DIR, \"mindbreath_model.h5\")\n",
    "DEFAULT_OUT = os.path.join(OUT_DIR, \"predictions_mindbreath.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper classes (must match training for safe unpickling)\n",
    "# -----------------------------\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column): self.column = column\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X[[self.column]]\n",
    "\n",
    "class To1DString(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.iloc[:, 0].astype(str).values\n",
    "        return np.asarray(X).astype(str).ravel()\n",
    "\n",
    "class DateTimeExpand(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns): self.columns = columns; self.out_cols = []\n",
    "    def fit(self, X, y=None):\n",
    "        self.out_cols = []\n",
    "        for c in self.columns:\n",
    "            self.out_cols += [f\"{c}_year\", f\"{c}_month\", f\"{c}_day\", f\"{c}_dow\", f\"{c}_hour\"]\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        outs = []\n",
    "        for c in self.columns:\n",
    "            s = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "            outs.append(pd.DataFrame({\n",
    "                f\"{c}_year\":  s.dt.year.fillna(0).astype(int),\n",
    "                f\"{c}_month\": s.dt.month.fillna(0).astype(int),\n",
    "                f\"{c}_day\":   s.dt.day.fillna(0).astype(int),\n",
    "                f\"{c}_dow\":   s.dt.dayofweek.fillna(0).astype(int),\n",
    "                f\"{c}_hour\":  s.dt.hour.fillna(0).astype(int),\n",
    "            }))\n",
    "        return pd.concat(outs, axis=1) if outs else np.empty((len(X), 0))\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def ensure_dense_if_small(X, max_feats=50000):\n",
    "    if hasattr(X, \"toarray\") and X.shape[1] <= max_feats:\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "def compile_loaded_model(model, n_classes: int):\n",
    "    # Not required for predict(), but silences TF warnings\n",
    "    if n_classes <= 2:\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def ensure_schema(df_in: pd.DataFrame, bundle: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure incoming DataFrame has the columns seen at training time.\n",
    "    Missing columns are filled with safe defaults by type.\n",
    "    Extra columns are fine; ColumnTransformer ignores them unless referenced.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    num_cols = bundle.get(\"numeric_cols\", [])\n",
    "    cat_cols = bundle.get(\"cat_cols\", [])\n",
    "    txt_cols = bundle.get(\"text_cols\", [])\n",
    "    dt_cols  = bundle.get(\"datetime_cols\", [])\n",
    "\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"unknown\"\n",
    "    for c in txt_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    for c in dt_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.Timestamp.today()\n",
    "\n",
    "    # Keep a stable order (optional but nice)\n",
    "    preferred = [*num_cols, *cat_cols, *txt_cols, *dt_cols]\n",
    "    front = [c for c in preferred if c in df.columns]\n",
    "    tail  = [c for c in df.columns if c not in front]\n",
    "    return df[front + tail] if front else df\n",
    "\n",
    "def build_df_for_text(text: str, bundle: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a 1-row DataFrame aligned to training schema, dropping the text\n",
    "    into the first text column (if any). If no text columns existed, create one.\n",
    "    \"\"\"\n",
    "    base = ensure_schema(pd.DataFrame(), bundle)\n",
    "    # convert 0-row to 1-row defaults\n",
    "    row = {}\n",
    "    for c in base.columns:\n",
    "        if c in bundle.get(\"numeric_cols\", []): row[c] = 0\n",
    "        elif c in bundle.get(\"cat_cols\", []):   row[c] = \"unknown\"\n",
    "        elif c in bundle.get(\"datetime_cols\", []): row[c] = pd.Timestamp.today()\n",
    "        else: row[c] = \"\"\n",
    "    df = pd.DataFrame([row])\n",
    "    txt_cols = bundle.get(\"text_cols\", [])\n",
    "    if txt_cols:\n",
    "        df.loc[0, txt_cols[0]] = text\n",
    "    else:\n",
    "        df[\"text\"] = text\n",
    "    return df\n",
    "\n",
    "def predict_dataframe(df_in: pd.DataFrame, bundle: dict, model, out_csv: str = None, print_out: bool = False) -> pd.DataFrame:\n",
    "    preprocess    = bundle[\"preprocess\"]\n",
    "    label_encoder = bundle[\"label_encoder\"]\n",
    "    classes       = [str(c) for c in label_encoder.classes_]\n",
    "    n_classes     = len(classes)\n",
    "\n",
    "    df_aligned = ensure_schema(df_in, bundle)\n",
    "    X = preprocess.transform(df_aligned)\n",
    "    X = ensure_dense_if_small(X)\n",
    "\n",
    "    probs = model.predict(X, verbose=0)\n",
    "\n",
    "    # Handle binary and multiclass probabilities\n",
    "    if probs.ndim == 1 or probs.shape[1] == 1:\n",
    "        pos = probs.ravel()\n",
    "        neg = 1.0 - pos\n",
    "        prob_mat = np.vstack([neg, pos]).T\n",
    "        pred_idx = (pos >= 0.5).astype(int)\n",
    "    else:\n",
    "        prob_mat = probs\n",
    "        pred_idx = np.argmax(prob_mat, axis=1)\n",
    "\n",
    "    pred_labels = label_encoder.inverse_transform(pred_idx)\n",
    "\n",
    "    out = df_in.copy()\n",
    "    out.insert(0, \"pred_label\", pred_labels)\n",
    "\n",
    "    # Append probabilities\n",
    "    if prob_mat.shape[1] == 2 and n_classes == 2:\n",
    "        out[\"prob_neg\"] = prob_mat[:, 0]\n",
    "        out[\"prob_pos\"] = prob_mat[:, 1]\n",
    "    else:\n",
    "        # Align prob columns with label order (truncate/min-match just in case)\n",
    "        k = min(prob_mat.shape[1], n_classes)\n",
    "        for j in range(k):\n",
    "            out[f\"prob_{classes[j]}\"] = prob_mat[:, j]\n",
    "\n",
    "    if out_csv:\n",
    "        out.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[SAVE] Predictions -> {out_csv}\")\n",
    "\n",
    "    if print_out:\n",
    "        cols = [\"pred_label\"] + [c for c in out.columns if c.startswith(\"prob_\")]\n",
    "        print(out[cols].head(min(20, len(out))).to_string(index=False))\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# CLI / Entry\n",
    "# -----------------------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"MindBreath Predictor\")\n",
    "    p.add_argument(\"--in\",   dest=\"in_csv\",   type=str, default=None, help=\"Input CSV (same schema as training)\")\n",
    "    p.add_argument(\"--out\",  dest=\"out_csv\",  type=str, default=None, help=\"Output predictions CSV path\")\n",
    "    p.add_argument(\"--text\", dest=\"single_text\", type=str, default=None, help=\"Single text to predict (if text columns existed)\")\n",
    "    p.add_argument(\"--print\", dest=\"do_print\", action=\"store_true\", help=\"Print predictions table\")\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "def main(argv=None):\n",
    "    # Make Jupyter safe: strip injected args like \"-f kernel-*.json\"\n",
    "    if argv is None and (\"ipykernel_launcher\" in sys.argv[0] or any(a == \"-f\" for a in sys.argv)):\n",
    "        argv = []\n",
    "\n",
    "    args = parse_args(argv)\n",
    "\n",
    "    # Interactive fallback when no args provided\n",
    "    if args.single_text is None and args.in_csv is None:\n",
    "        try:\n",
    "            s = input(\"Enter a sentence to predict (or leave blank to use a CSV): \").strip()\n",
    "        except EOFError:\n",
    "            s = \"\"\n",
    "        if s:\n",
    "            args.single_text = s\n",
    "        else:\n",
    "            csv_path = input(\"Enter path to input CSV (or press Enter to cancel): \").strip()\n",
    "            if not csv_path:\n",
    "                raise ValueError(\"Provide either --text 'some sentence' or --in path\\\\to\\\\file.csv\")\n",
    "            args.in_csv = csv_path\n",
    "\n",
    "    # Load artifacts\n",
    "    if not os.path.exists(PKL_PATH):\n",
    "        raise FileNotFoundError(f\"Missing preprocess bundle: {PKL_PATH}\")\n",
    "    if not os.path.exists(H5_PATH):\n",
    "        raise FileNotFoundError(f\"Missing model file: {H5_PATH}\")\n",
    "\n",
    "    bundle = joblib.load(PKL_PATH)\n",
    "    model  = load_model(H5_PATH)\n",
    "    compile_loaded_model(model, len(bundle[\"label_encoder\"].classes_))\n",
    "\n",
    "    # Build input dataframe\n",
    "    if args.single_text is not None:\n",
    "        df_in = build_df_for_text(args.single_text, bundle)\n",
    "    else:\n",
    "        if not os.path.exists(args.in_csv):\n",
    "            raise FileNotFoundError(f\"Input CSV not found: {args.in_csv}\")\n",
    "        df_in = pd.read_csv(args.in_csv)\n",
    "\n",
    "    out_csv = args.out_csv or DEFAULT_OUT\n",
    "    predict_dataframe(df_in, bundle, model, out_csv=out_csv, print_out=args.do_print)\n",
    "\n",
    "# -----------------------------\n",
    "# Optional notebook helpers\n",
    "# -----------------------------\n",
    "def predict_text(text: str, save_to: str = None, show=True):\n",
    "    bundle = joblib.load(PKL_PATH)\n",
    "    model  = load_model(H5_PATH)\n",
    "    compile_loaded_model(model, len(bundle[\"label_encoder\"].classes_))\n",
    "    df_in = build_df_for_text(text, bundle)\n",
    "    return predict_dataframe(df_in, bundle, model, out_csv=save_to, print_out=show)\n",
    "\n",
    "def predict_csv(csv_path: str, save_to: str = None, show=True):\n",
    "    bundle = joblib.load(PKL_PATH)\n",
    "    model  = load_model(H5_PATH)\n",
    "    compile_loaded_model(model, len(bundle[\"label_encoder\"].classes_))\n",
    "    df_in = pd.read_csv(csv_path)\n",
    "    return predict_dataframe(df_in, bundle, model, out_csv=save_to, print_out=show)\n",
    "\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaced27-67eb-4198-bfd8-dc9617bbb72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
